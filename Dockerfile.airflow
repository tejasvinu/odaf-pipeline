FROM apache/airflow:2.5.0

USER root

# Install required packages and bash explicitly for spark-submit
RUN apt-get update && \
    apt-get install -y \
    netcat-traditional \
    netcat \
    wget \
    kafkacat \
    procps \
    curl \
    bash && \
    wget https://dl.min.io/client/mc/release/linux-amd64/mc && \
    chmod +x mc && \
    mv mc /usr/local/bin/ && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins /opt/airflow/config && \
    chown -R airflow:root /opt/airflow && \
    # Create nc symlink if it doesn't exist
    if [ ! -f /usr/bin/nc ]; then ln -s /usr/bin/netcat /usr/bin/nc; fi

# Install Java correctly
RUN apt-get update && \
    apt-get install -y openjdk-11-jdk && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME environment variable properly
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:${PATH}"

# Create symlink and verify Java installation
RUN ln -sf /usr/lib/jvm/java-11-openjdk-amd64 /usr/lib/jvm/default-java && \
    echo "Symlinked java-11-openjdk-amd64 to /usr/lib/jvm/default-java" && \
    which java && \
    java -version && \
    ln -s /bin/ps /usr/bin/ps 2>/dev/null || true && \
    # Make Java environment globally available
    echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /etc/profile && \
    echo "export PATH=$JAVA_HOME/bin:$PATH" >> /etc/profile && \
    # Make Java environment available to airflow user
    echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /home/airflow/.bashrc && \
    echo "export PATH=$JAVA_HOME/bin:$PATH" >> /home/airflow/.bashrc

# Copy and prepare scripts for environment verification and setup
COPY verify_environment.sh /opt/airflow/verify_environment.sh
COPY fix_bash_issue.sh /opt/airflow/fix_bash_issue.sh
COPY fix_airflow_connections.sh /opt/airflow/fix_airflow_connections.sh
COPY fix_spark_issues.sh /opt/airflow/fix_spark_issues.sh
COPY airflow-entrypoint.sh /entrypoint.sh

# Clean up script files and ensure they're executable
RUN sed -i 's/\r$//' /opt/airflow/verify_environment.sh && \
    sed -i 's/\r$//' /opt/airflow/fix_bash_issue.sh && \
    sed -i 's/\r$//' /opt/airflow/fix_airflow_connections.sh && \
    sed -i 's/\r$//' /opt/airflow/fix_spark_issues.sh && \
    sed -i 's/\r$//' /entrypoint.sh && \
    chmod +x /opt/airflow/verify_environment.sh && \
    chmod +x /opt/airflow/fix_bash_issue.sh && \
    chmod +x /opt/airflow/fix_airflow_connections.sh && \
    chmod +x /opt/airflow/fix_spark_issues.sh && \
    chmod +x /entrypoint.sh

# Set up spark-submit and other spark binaries correctly
# This is updated to ensure compatibility with newer Airflow versions
RUN mkdir -p /usr/local/bin && \
    # Create a script that properly sets up spark-submit at container startup
    echo '#!/bin/bash' > /usr/local/bin/setup-spark-submit && \
    echo 'set -e' >> /usr/local/bin/setup-spark-submit && \
    echo 'mkdir -p /home/airflow/.local/bin' >> /usr/local/bin/setup-spark-submit && \
    echo 'PYSPARK_DIR=$(python -c "import pyspark; print(pyspark.__path__[0])" 2>/dev/null)' >> /usr/local/bin/setup-spark-submit && \
    echo 'if [ -f "$PYSPARK_DIR/bin/spark-submit" ]; then' >> /usr/local/bin/setup-spark-submit && \
    echo '  ln -sf "$PYSPARK_DIR/bin/spark-submit" /home/airflow/.local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'else' >> /usr/local/bin/setup-spark-submit && \
    echo '  echo "#!/bin/bash" > /home/airflow/.local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo '  echo "python -m pyspark.bin.spark-submit \"\$@\"" >> /home/airflow/.local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo '  chmod +x /home/airflow/.local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'fi' >> /usr/local/bin/setup-spark-submit && \
    echo 'chmod +x /home/airflow/.local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'echo "#!/bin/bash" > /usr/local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'echo "exec /home/airflow/.local/bin/spark-submit \"\$@\"" >> /usr/local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'chmod +x /usr/local/bin/spark-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'ln -sf /usr/local/bin/spark-submit /usr/local/bin/spark2-submit' >> /usr/local/bin/setup-spark-submit && \
    echo 'ln -sf /usr/local/bin/spark-submit /usr/local/bin/spark3-submit' >> /usr/local/bin/setup-spark-submit && \
    chmod +x /usr/local/bin/setup-spark-submit && \
    # Run the setup script during build time
    /usr/local/bin/setup-spark-submit

# Explicitly set directory permissions
RUN mkdir -p /opt/airflow/{logs,dags,plugins,config,downloads} && \
    chmod -R 777 /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins /opt/airflow/config /opt/airflow/downloads && \
    touch /opt/airflow/.airflowignore && chmod 644 /opt/airflow/.airflowignore

USER airflow

# Set environment variables to help Python find modules
ENV PYTHONPATH=/opt/airflow:/home/airflow/.local/lib/python3.7/site-packages:${PYTHONPATH}
ENV PATH=/home/airflow/.local/bin:/usr/local/bin:${PATH}

# Ensure proper Airflow installation during build time with constraints
RUN pip install --upgrade pip && \
    pip config set global.timeout 1000 && \
    pip config set global.retries 10 && \
    # Download constraints file with retries to avoid network issues
    wget --tries=5 --retry-connrefused -O /tmp/constraints.txt "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.0/constraints-3.7.txt" && \
    # Force reinstall airflow with constraints to ensure everything is properly installed
    pip uninstall -y apache-airflow || true && \
    pip install --no-cache-dir --user "apache-airflow==2.5.0" --constraint "/tmp/constraints.txt" && \
    # Verify Airflow is properly installed
    python -c "import airflow; print(f'Installed Airflow version: {airflow.__version__}')" && \
    # Clean up
    rm -f /tmp/constraints.txt

# Install all dependencies at build time
RUN pip install --no-cache-dir --user \
    boto3 \
    kafka-python \
    requests \
    pyspark==3.3.2 \
    apache-airflow-providers-apache-spark==4.1.0 \
    psycopg2-binary && \
    # Verify all packages are properly installed
    pip list | grep -E "boto3|kafka-python|requests|pyspark|apache-airflow-providers-apache-spark|psycopg2-binary" && \
    # Initialize the airflow database during build to avoid first-run delays
    mkdir -p ${AIRFLOW_HOME}/logs ${AIRFLOW_HOME}/dags && \
    airflow version

# Add Java and Airflow paths for airflow user's shell
RUN echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/.bashrc && \
    echo "export PYTHONPATH=/opt/airflow:/home/airflow/.local/lib/python3.7/site-packages:\${PYTHONPATH}" >> ~/.bashrc && \
    echo "export PATH=/usr/local/bin:/home/airflow/.local/bin:\$JAVA_HOME/bin:\$PATH" >> ~/.bashrc && \
    # Run the spark setup script again as airflow user to ensure proper permissions
    /usr/local/bin/setup-spark-submit

# Set up Airflow connections - use newer format without deprecated spark-home
ENV AIRFLOW_CONN_SPARK_DEFAULT="spark://local[*]?spark-binary=spark-submit"

# Make sure airflow is in the PATH
ENV PATH="/usr/local/bin:/home/airflow/.local/bin:${PATH}"

# Set workdir to airflow home
WORKDIR /opt/airflow

# Use our custom entrypoint
ENTRYPOINT ["/entrypoint.sh"]